# ðŸ§  Factual Chatbot with Context Tracking

A sophisticated command-line chatbot that maintains conversation context and answers questions about capitals, currencies, presidents, prime ministers, and more using a combination of structured data and AI.

## Overview

This chatbot demonstrates advanced context handling in conversational AI. It seamlessly combines:
- **Structured factual data** for precise, deterministic answers
- **Hugging Face's FLAN-T5-Large model** for open-ended questions
- **Smart context tracking** that remembers conversation flow
- **Follow-up question handling** for natural conversations

## Key Features

### ðŸŽ¯ Intelligent Context Tracking
- Maintains conversation context across multiple turns
- Understands follow-up questions like "and what about USA?" or "and pakistan"
- Automatically reconstructs full questions from short follow-ups
- Never loses track of what you're asking about

### ðŸ“Š Multi-Domain Knowledge
- **Capitals**: 13+ countries
- **Currencies**: 15+ countries
- **Leaders**: Presidents and Prime Ministers of major nations
- **General Knowledge**: Falls back to FLAN-T5 model for anything else

### ðŸ”„ Smart Question Handling
- Detects question type (capital, currency, president, prime minister)
- Maintains question context for follow-ups
- Reconstructs ambiguous questions automatically
- Falls through to AI model when data not in database

### ðŸ’¾ Conversation Memory
- Keeps track of last 4 conversation turns
- Uses context to improve model responses
- Prevents context confusion between question types

## Installation

```bash
pip install transformers torch sentencepiece
```

## Usage

```bash
python interface.py
```

Type `/exit` to quit.

## Examples

### Example 1: Currency Questions with Follow-ups
```
User: what is the currency of india
Bot: Indian rupee

User: and pakistan
Bot: Pakistani rupee

User: and USA
Bot: United States dollar

User: and what about oman
Bot: Omani rial
```

### Example 2: Context Switching (Currency â†’ President)
```
User: what is the currency of india
Bot: Indian rupee

User: who is the president of oman
Bot: Sultan Haitham bin Tariq

User: and USA
Bot: Joe Biden    # Correctly uses PRESIDENT context, not currency!

User: and france
Bot: Emmanuel Macron
```

### Example 3: Prime Minister Questions
```
User: who is the prime minister of india
Bot: Narendra Modi

User: and pakistan
Bot: Shehbaz Sharif

User: and what about japan
Bot: Fumio Kishida

User: and UK
Bot: Rishi Sunak
```

### Example 4: Capital Questions
```
User: what is the capital of france
Bot: Paris

User: and italy
Bot: Rome

User: and canada
Bot: Ottawa
```

## How It Works

### Context Tracking System

1. **Question Type Detection**: Identifies whether you're asking about capitals, currencies, presidents, or prime ministers
2. **Follow-up Recognition**: Detects patterns like "and X", "what about X", "and what about X"
3. **Question Reconstruction**: Transforms "and pakistan" into full question based on context
   - After currency question: "what is the currency of pakistan"
   - After president question: "who is the president of pakistan"
4. **Context Preservation**: Maintains `last_question_type` to ensure follow-ups use correct context

### Fallback to AI Model

When data isn't in the database:
- Question is sent to FLAN-T5-Large model
- Model uses conversation history for context
- Provides best-effort answer based on training data

## Project Structure

```
factual_chatbot_chatgpt-2/
â”œâ”€â”€ interface.py          # Main chatbot interface with context tracking
â”œâ”€â”€ model_loader.py       # Loads and configures FLAN-T5-Large model
â”œâ”€â”€ chat_memory.py        # Manages conversation history
â”œâ”€â”€ changes+bugs/         # Detailed documentation of bug fixes
â”‚   â””â”€â”€ documentation.txt # Complete bug analysis and fixes (35KB)
â””â”€â”€ README.md            # This file
```

## Technical Details

### Model
- **Name**: google/flan-t5-large
- **Type**: Text-to-text transformer
- **Parameters**: 780M
- **Device**: CPU (no GPU required)
- **Max tokens**: 150 per response

### Data Coverage
- **Capitals**: India, Italy, France, Germany, Spain, UK, China, Japan, Canada, Australia, Russia, USA, Oman
- **Currencies**: 15+ countries including India, USA, UK, Japan, China, Pakistan, Oman, and European countries
- **Leaders**: Presidents and Prime Ministers of India, USA, France, UK, Canada, Russia, Japan, Pakistan, China

### Context Window
- Maintains last 4 conversation turns
- Tracks current question type (capital, currency, president, prime_minister)
- Reconstructs follow-up questions automatically

## Bug Fixes & Improvements

This project underwent extensive debugging and improvement across 4 passes:

### Major Issues Fixed
1. âœ… Missing data entries (Pakistan PM, China leaders, currencies)
2. âœ… Broken regex patterns for follow-up detection
3. âœ… Key mismatch issues (underscore vs space)
4. âœ… Currency detection system created from scratch
5. âœ… Question type tracking for model responses
6. âœ… Error messages blocking model fallback
7. âœ… Follow-up question reconstruction

**Total**: 9 major bugs fixed, 145+ lines of code changed

For complete technical details, see [`changes+bugs/documentation.txt`](changes+bugs/documentation.txt)

## Development Notes

### Adding New Data

**To add a capital:**
```python
CAPITALS = {
    "new_country": "Capital City",
    # ...
}
```

**To add a currency:**
```python
CURRENCIES = {
    "new_country": "Currency Name",
    # ...
}
```

**To add a leader:**
```python
FACTUALS = {
    "president of new_country": "Leader Name",
    "prime minister of new_country": "Leader Name",
    # ...
}
```

### Testing

The project includes comprehensive testing:
- Follow-up detection tests
- Context tracking verification
- Question reconstruction validation
- Model fallback testing

## Known Limitations

1. **Model Knowledge Cutoff**: FLAN-T5 training data has a cutoff date; recent events may not be known
2. **Data Coverage**: Only includes major countries; expand dictionaries for more coverage
3. **CPU Speed**: Model runs on CPU; may be slower than GPU
4. **Memory**: Keeps only last 4 turns; older context is forgotten

## Future Improvements

- [ ] Add more countries and leaders
- [ ] Include historical leaders with date ranges
- [ ] Support more question types (populations, languages, etc.)
- [ ] Add caching for frequently asked questions
- [ ] Implement confidence scores for model responses
- [ ] Add source attribution for factual answers

## Contributing

When adding new features or data:
1. Test with follow-up questions to ensure context tracking works
2. Update both structured data (dictionaries) and documentation
3. Run through multiple conversation scenarios
4. Document any new bug fixes in `changes+bugs/`

## License

This project is open source and available for educational purposes.

## Acknowledgments

- **Hugging Face** for FLAN-T5 model
- **Google** for developing FLAN-T5
- **PyTorch** and **Transformers** libraries

---

**Status**: âœ… All context tracking bugs resolved (4 debugging passes completed)
**Last Updated**: October 25, 2025
**Bugs Fixed**: 9 major issues
**Code Changes**: ~145 lines
